{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HRV_Features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUZGu25RkyNB",
        "outputId": "bf5b3c49-c780-442a-f7bb-11bcd3a27178"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UIfaoynk4G9",
        "outputId": "71b53580-c145-4b87-c934-93012bb4cf29"
      },
      "source": [
        "pip install neurokit2 # install neurokit which is a python module for signal processing"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neurokit2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/4a/d2a9502942cb60e61c9ba9772c04ebd0a945fe248ed42cb520334da582b2/neurokit2-0.1.1-py2.py3-none-any.whl (990kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.1.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->neurokit2) (1.15.0)\n",
            "Installing collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvNp3qt8lCzs",
        "outputId": "cb1312ea-c9c3-4c61-e97a-2b8720c6bb6d"
      },
      "source": [
        "pip install biosppy==0.6.1 # install biosppy which is a python module for signal processing"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting biosppy==0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/60/d09a277f9d31a2fc9190edf7e8a685c4f9b54b5dff487f523b916f441e1a/biosppy-0.6.1-py2.py3-none-any.whl (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 10.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.19.5)\n",
            "Collecting bidict\n",
            "  Downloading https://files.pythonhosted.org/packages/67/d4/eaf9242722bf991e0955380dd6168020cb15a71cc0d3cc2373f4911b1f1d/bidict-0.21.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.4.1)\n",
            "Collecting shortuuid\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->biosppy==0.6.1) (1.0.1)\n",
            "Installing collected packages: bidict, shortuuid, biosppy\n",
            "Successfully installed bidict-0.21.2 biosppy-0.6.1 shortuuid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7cP-1kFlDPC",
        "outputId": "f99c930c-9343-4ec9-aa5d-4b69fc988f35"
      },
      "source": [
        "pip install mne# install mne which is a python module for signal processing"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f7/2bf5de3fad42b66d00ee27539bc3be0260b4e66fdecc12f740cdf2daf2e7/mne-0.23.0-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.23.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_YrkhdlF2n"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time \n",
        "import re\n",
        "import csv\n",
        "import scipy.io\n",
        "import biosppy\n",
        "import mne\n",
        "import neurokit2 as nk\n",
        "import ast\n",
        "import os\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import glob\n",
        "from scipy.stats import zscore, norm\n",
        "from neurokit2 import eda_phasic\n",
        "from scipy.stats import linregress\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h68_zi4IlKBz"
      },
      "source": [
        "# this function is to convert the TimeStamp column (first column) from Unix Epoch time to standard datetime format\n",
        "def TimeStamp_Conversion(ts):\n",
        "  \"\"\"\n",
        "  we have a unix epoch time in milliseconds i.e, a string with a length of 13 charcters example:1.5789360034388428E12\n",
        "  \n",
        "  parameters:\n",
        "  -----\n",
        "  ts = Epoch timesatmp in milliseconds.\n",
        "\n",
        "  Returns:\n",
        "  -----\n",
        "  Std_Unix = standard epoch timestamp in seconds.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  float_Input = float(ts) # converting the string-type(1.5789360034388428E12) Unix Epoch to float-type(1578936003.4388428).\n",
        "\n",
        "  # float input is divided by 1000 to convert the Unix epoch in milliseconds to seconds \n",
        "  Std_Unix = float_Input/1000\n",
        "\n",
        "  datetime_Input = datetime.fromtimestamp(Std_Unix) \n",
        "  # datetime.fromtimestamp converts the unix epoch in seconds to datetime returns example:datetime.datetime(2020, 1, 13, 17, 20, 3, 438843)\n",
        "\n",
        "  return Std_Unix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEIBiWCDlNC9"
      },
      "source": [
        "def column_formatting(Timestamp_DF):\n",
        "  \"\"\"\n",
        "  Column names of Timestamp annotation excel have column index attached to column name as we only need column name we are parsing column names.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  Timestamp_DF = Input the DF after reading the timestamp annotationexcel file to  get list of column names['A1- ECG baseline start','B1- ECG baseline end',.....].\n",
        "\n",
        "  Returns:\n",
        "  -----\n",
        "  Parsed_ColumnNames = list of parsed column names. ['ECG baseline start','ECG baseline end',....]\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Parsed_colnames = ['Subject_ID'] ## Column with Participant ID is not named, so declaring first column as Subject_ID to an empty list\n",
        "\n",
        "  for index in range(1,len(Timestamp_DF.columns)): ## Looping through the list of timestamp annotation columns list\n",
        "    column = Timestamp_DF.columns[index][4:].lstrip() ## Drop first 3 indices of each column and strip space(\" \") if present as left most\n",
        "    Parsed_colnames.append(column) ## appending each column name after parsing\n",
        "\n",
        "  return Parsed_colnames ## returns list fo parsed col names"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB6_LPQZlP3Z"
      },
      "source": [
        "def Annotation_timestamp(timestamp_path, sheet_name):\n",
        "  \"\"\"\n",
        "  This function is to change the column names of timestamp annotations table and convert timestamps from milliseconds to standart epoch format of seconds.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  timestamp_path = path to the directory of file location\n",
        "  sheet_name =  there are two sheets present in the file, we work on file named D.\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  VR_Timestamps_D = Clean dataframe of timestamp annotations table.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  VR_TimeStamps_D = pd.read_excel(Timestamp_path, sheet_name) ## read timestamp annotation file\n",
        "  Parsed_colnames = column_formatting(VR_TimeStamps_D) ## using the column_formatting function defined earlier parse columns\n",
        "  VR_TimeStamps_D.columns = Parsed_colnames ## Change colnames of Dataframe using the parsed list of col names\n",
        "  \n",
        "  ## As timestamp is in string format and in milli seconds iterating through each column to change the timestamp to standard epoch format.\n",
        "  for col in VR_TimeStamps_D.columns: \n",
        "    ## Using Timestamp_Conversion function and lambda fucntion to map the function to each row of the column.\n",
        "    if col == 'Subject_ID':\n",
        "      pass\n",
        "    else:\n",
        "      VR_TimeStamps_D[col] = VR_TimeStamps_D[col].map(lambda instance: TimeStamp_Conversion(instance)) \n",
        "\n",
        "  return VR_TimeStamps_D"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2pnPPtqlUzH"
      },
      "source": [
        "def Shimmers_csv2DF(path,filename):\n",
        "  \"\"\"\n",
        "  This function is to read Shimmer data files and create a dataframe from tidy shimmers csv tables.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  path = path to directory of shimmers file folder.\n",
        "\n",
        "  filename = name of the file to be loaded.\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  Dataframe = organized and structured Shimmers Data.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  with open(path + '/' + filename, 'r',) as file: # read the file\n",
        "    reader = csv.reader(file)\n",
        "\n",
        "    lists_eachrow = []\n",
        "    for row in reader:\n",
        "      lists_eachrow.append(row) # append each row in reader to a list\n",
        "\n",
        "  del lists_eachrow[0] # del first row of list as it is only about \\t delimiter used\n",
        "\n",
        "  newlists = [] \n",
        "  # loop through the list of lists and split columnar values using the delimiter \n",
        "  for list_row in lists_eachrow:\n",
        "    for row in list_row:\n",
        "      newlists.append(list(row.split('\\t')))\n",
        "  # Extract subjectID from the file name for future use\n",
        "  filename_parse = filename.replace(\"_\", \" \")\n",
        "  Participant_ID = ast.literal_eval(re.findall(r'\\b\\d+\\b', filename_parse)[0])\n",
        "  \n",
        "  # create dataframe from the list of columnar values \n",
        "  Dataframe = pd.DataFrame(newlists, columns = newlists[0])\n",
        "  Dataframe = Dataframe.drop([0,1]) # drop columns 1 and 2 which are column names and units as we already have column names for new dataframe.\n",
        "  Dataframe.reset_index(drop=True, inplace=True) # reset index\n",
        "\n",
        "  return Dataframe, Participant_ID "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WF5-SWi4KCz"
      },
      "source": [
        "# IBI(inter beat interval or RR interval) column in the ECG data consits of location of R-peaks and the ECG sample at each R peak location is given as an input to HRV function to extract features\n",
        "# This function extrcats the indices where R peak is located in IBI columns.\n",
        "def Rpeak_Indices(Dataframe):\n",
        "  Rpeak_Indices = []\n",
        "  for i in Dataframe.columns:\n",
        "    if 'IBI' in i:\n",
        "      index_list = []\n",
        "      index_list = Dataframe[i].index[Dataframe[i] != -1].tolist()\n",
        "      Rpeak_Indices += index_list\n",
        "  Rpeak_Indices = np.unique(Rpeak_Indices)\n",
        "  return Rpeak_Indices"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_v526RblZeI"
      },
      "source": [
        "# ECG Data consists of 4 different electrodes collecting data from 4 limbs (LA, RA, RL and LL)\n",
        "# It has raw ECG signal, IBI and Heart Rate signals with each of 4 columns\n",
        "# HRV is Heart Rate Variability, low HRV indicates high stress\n",
        "# IBI is Inter-Beat Interval or RR interval is the duration between two R peaks\n",
        "# \n",
        "# Function to extract HRV features from ECG signal\n",
        "\n",
        "def ECG_features(path,Timestamp_path,sheet_name = 'D'):\n",
        "  # taking path location of Shimmer file as input and using glob function to find csv and text files in folder and appending each subject's filename to a list\n",
        "  os.chdir(path)\n",
        "  list_csv = glob.glob('*.{}'.format('txt'))\n",
        "  list_csv.extend(glob.glob('*.{}'.format('csv')))\n",
        "  \n",
        "  # Creating an empty DataFrame to append all the features\n",
        "  Data = pd.DataFrame()\n",
        "\n",
        "  # loop to feature extraction on each subject file.\n",
        "  for index in list_csv:\n",
        "    Dataframe, Participant_ID = Shimmers_csv2DF(path,index) # formatting csv file to create a clean DataFrame and extract Participant ID from file name using Shimmers_csv2DF function\n",
        "    # Convert the timestamp column in nanoseconds to milliseconds\n",
        "    Dataframe['Shimmer_CB7C_Timestamp_Unix_CAL'] = Dataframe['Shimmer_CB7C_Timestamp_Unix_CAL'].map(lambda instance: TimeStamp_Conversion(instance))\n",
        "    \n",
        "    ''' \n",
        "    --- One of the Shimmer file with Subject ID 877 do not have IBI columns, we need to extract the location of R peaks using the nk.ecg_peaks()\n",
        "        function from neurokit ECG processing module. After getting R-peak locations we need to extract ECG samples for those R-peak locations.\n",
        "\n",
        "    --- For those files with IBI columns we need to pass those columns through IBI_indices function to extract indices of R peak location a get ECG\n",
        "        samples for those R-peak locations.      \n",
        "    '''\n",
        "\n",
        "    try:\n",
        "\n",
        "      # ECG columns in the shimmer data are in string type, below loop itetrate through each row of all the ECG columns to cast string type to int/float\n",
        "      for index in range(0,5):\n",
        "        columns = ['Shimmer_CB7C_ECG_IBI_LA_RA_CAL',\n",
        "                   'Shimmer_CB7C_ECG_IBI_LL_LA_CAL', \n",
        "                   'Shimmer_CB7C_ECG_IBI_LL_RA_CAL',\n",
        "                   'Shimmer_CB7C_ECG_IBI_Vx_RL_CAL',\n",
        "                   'Shimmer_CB7C_ECG_LA-RA_24BIT_CAL']\n",
        "        Dataframe[columns[index]] = Dataframe[columns[index]].map(lambda ind : ast.literal_eval(ind))\n",
        "\n",
        "      # Extract indices where we have R peaks \n",
        "      Rpeak_loc = Rpeak_Indices(Dataframe)\n",
        "\n",
        "      # Dataframe with rows where R peaks are located\n",
        "      Dataframe_Rpeak = Dataframe.iloc[Rpeak_loc]\n",
        " \n",
        "      # creating a numpy array of raw ECG signal from column LA_RA lead(one of four electrodes and is closer to heart) where we have R peaks\n",
        "      ecg = np.array(Dataframe_Rpeak['Shimmer_CB7C_ECG_LA-RA_24BIT_CAL'])\n",
        "\n",
        "      # For the file with out IBI column will result in KeyError, below colde helps to extract R peak locations from raw ECG signal using n.ecg_peaks \n",
        "    except KeyError:\n",
        "\n",
        "      # empty list to append peaks extracted from signals of four electrodes\n",
        "      Rpeaks_index = []\n",
        "\n",
        "      # ECG columns in the shimmer data are in string type, below loop itetrate through each row of all the ECG columns to cast string type to int/float\n",
        "      for index in range(0,4):\n",
        "           \n",
        "        columns = ['Shimmer_CB7C_ExG1_CH1_24BIT_CAL', 'Shimmer_CB7C_ExG1_CH2_24BIT_CAL',\n",
        "       'Shimmer_CB7C_ExG2_CH1_24BIT_CAL', 'Shimmer_CB7C_ExG2_CH2_24BIT_CAL']\n",
        "        \n",
        "        Dataframe[columns[index]] = Dataframe[columns[index]].map(lambda ind : ast.literal_eval(ind))\n",
        "\n",
        "        # each column is cleaned and R peaks locations(indices) are extracted  and appended to peaks_index\n",
        "        ecg = Dataframe[columns[index]]\n",
        "        cleaned = nk.ecg_clean(ecg, sampling_rate=128)\n",
        "        signals, info = nk.ecg_peaks(cleaned, sampling_rate=128, correct_artifacts=True)\n",
        "        peaks = info['ECG_R_Peaks']\n",
        "        for i in peaks:\n",
        "          Rpeaks_index.append(i)\n",
        "      \n",
        "      # as we are looking for R peaks location in four different signals we may have duplicates when we append all of them together, so removing duplicates.\n",
        "      Rpeaks_index = np.unique(Rpeaks_index)\n",
        "      Rpeaks_index = np.array(Rpeaks_index) # creating an array of R peak indices\n",
        "\n",
        "      # Dataframe with rows where R peaks are located\n",
        "      Dataframe_Rpeak = Dataframe.iloc[Rpeaks_index]\n",
        "\n",
        "      # creating a numpy array of raw ECG signal where we have R peaks from LA_RA lead(one of four electrodes and is closer to heart) called as channel 1 in this file\n",
        "      ecg = np.array(Dataframe_Rpeak['Shimmer_CB7C_ExG1_CH1_24BIT_CAL'])\n",
        "      \n",
        "    # numpy array of timestamp column with indices where R peaks are located\n",
        "    timestamp = np.array(Dataframe_Rpeak['Shimmer_CB7C_Timestamp_Unix_CAL'])\n",
        "\n",
        "    '''\n",
        "    --- As the neurokit function we are using to extract HRV features will only give one row as output when a signal is passed as input\n",
        "    --- I am using sliding window technique to extract features for every 60 data points(window length) and an overlap of 30 data points\n",
        "    --- we can expect average of one R peak for every second(i.e, 128Hz), as we are using 60 data points in each window means we are windowing for every 60 seconds with a 30 second overlap \n",
        "    '''\n",
        "    start_window=0\n",
        "    overlap=30\n",
        "    window_length=60\n",
        "    end_window = window_length\n",
        "\n",
        "    data = pd.DataFrame() # empty dataframe to append HRV features for each window\n",
        "    while (start_window + overlap) <= len(ecg): # loop conditon to carry out windowing and calculate features till the end \n",
        "      \n",
        "      # creating start, mean and end timestamps helps to  make sure that each window completely falls in annotation period\n",
        "      Start_timestamp = timestamp[start_window]\n",
        "      Mean_timestamp = np.mean(timestamp[start_window:end_window])\n",
        "      try:\n",
        "        end_timestamp = timestamp[end_window]\n",
        "      except IndexError: \n",
        "        # index of last window might exceed the last timestamp of our data, if this happens it will use last timestamp of data as end timsatmp of last window\n",
        "        end_timestamp = timestamp[-1]\n",
        "\n",
        "      ecg_window = ecg[start_window:end_window] # getting ecg samples for each winodw\n",
        "\n",
        "      hrv = nk.hrv_time(ecg_window, sampling_rate=128, show=False) # This takes ecg signal input for each window and gives each feature output as data frame column  \n",
        "      \n",
        "      # appending start, mean and end timestamp which helps in labelling event periods to each efature column\n",
        "      hrv['Start_timestamp'] = Start_timestamp \n",
        "      hrv['Mean_timestamp'] = Mean_timestamp\n",
        "      hrv['end_timestamp'] = end_timestamp\n",
        "      hrv['subject_ID'] = Participant_ID\n",
        "      \n",
        "      # append HRV features from each window to empty datafrme\n",
        "      data = data.append(hrv)\n",
        "\n",
        "      # increasing window length\n",
        "      start_window += overlap\n",
        "      end_window += overlap\n",
        "\n",
        "    data = data.dropna() # drop null values if any, as few rows consists of nulls values and infinity values while computing features\n",
        "    Data = Data.append(data) # appending HRV features of each subject to an empty DataFrame\n",
        "  \n",
        "  return Data  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZgPg8K06Q33"
      },
      "source": [
        "path = '/content/gdrive/My Drive/Food_VR/Food_VR/ECG_Processing/ECG_DATA/'\n",
        "Timestamp_path = '/content/gdrive/My Drive/Food_VR/Food_VR/VR Timestamps for Phase B & D_W&SP20.xlsx'\n",
        "Data = ECG_features(path,Timestamp_path,sheet_name = 'D')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOoe-BJ-cseC"
      },
      "source": [
        "Data.to_csv('/content/gdrive/My Drive/Food_VR/Food_VR/ECG_Processing/HRV_features.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5CD_VI4SvPR"
      },
      "source": [
        "Timestamp_path = '/content/gdrive/My Drive/Food_VR/Food_VR/VR Timestamps for Phase B & D_W&SP20.xlsx' # Event period Timestamp table"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5DpgKrg4ae2"
      },
      "source": [
        "Data = pd.read_csv('/content/gdrive/My Drive/Food_VR/Food_VR/ECG_Processing/HRV_features.csv') \n",
        "VR_TimeStamps_D = Annotation_timestamp(Timestamp_path, sheet_name = 'D') # passing timestamp data table path in to Annotation_timestamp to create a clean dataframe"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fBACWWMz8aU"
      },
      "source": [
        "# This function helps to label each row of HRV feature dataframe based on start, and end timestamp assoisated with it\n",
        "def Label_Data(Data,VR_TimeStamps_D):\n",
        "  list_subject = np.unique(Data['subject_ID']) # list out unique subjects\n",
        "  \n",
        "  # for each subject present in subject list we take out start and end time stamps of each event period \n",
        "  for index in list_subject:\n",
        "\n",
        "    VRBaseline_start = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == index,'VR baseline start'].iloc[0]\n",
        "    VRBaseline_end = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == index,'VR baseline end'].iloc[0]\n",
        "    Speech_start = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == index,'Emotion-induction speech start'].iloc[0]\n",
        "    Speech_end = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == index,'Emotion-induction speech end'].iloc[0]\n",
        "    Food_start = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == index,'Food selection start'].iloc[0]\n",
        "    Food_end = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == index,'food selection end'].iloc[0]\n",
        "\n",
        "    # using the start and end timestamps of each event period we check whether the start and end timestamps of ecah feature row fall in event start and end, if yes it is labelled as particular event and if not check for other event\n",
        "    # if the feature start and end timestamp does not lie in any of the vents start and end timestamp it results in NAN label \n",
        "    for i in range(0,len(Data)):\n",
        "      if ((VRBaseline_start <= Data['Start_timestamp'][i] <= VRBaseline_end) and (VRBaseline_start <= Data['Start_timestamp'][i] <= VRBaseline_end) and (VRBaseline_start <= Data['Start_timestamp'][i] <= VRBaseline_end)).all():\n",
        "        Data.loc[i,'Event'] = 'VR baseline'\n",
        "      elif (Speech_start <= Data['Start_timestamp'][i] <= Speech_end and Speech_start <= Data['Mean_timestamp'][i] <= Speech_end and Speech_start <= Data['end_timestamp'][i] <= Speech_end).all():\n",
        "        Data.loc[i,'Event'] = 'Speech Emotion'\n",
        "      elif (Food_start <= Data['Start_timestamp'][i] <= Food_end and Food_start <= Data['Mean_timestamp'][i] <= Food_end and Food_start <= Data['end_timestamp'][i] <= Food_end).all():\n",
        "        Data.loc[i,'Event'] = 'Food Selection'\n",
        "      else:\n",
        "        pass\n",
        "     \n",
        "  return Data # labelled Feature Data\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HbRpsxlz8cN"
      },
      "source": [
        "Data = Label_Data(Data,VR_TimeStamps_D)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4kD25Fuz9FZ"
      },
      "source": [
        "Data.to_csv(\"HRV_features_Label.csv\", index = False) # save labelled data"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdI7V4it84CY"
      },
      "source": [
        "DataFrame = Data.drop(['end_timestamp','Start_timestamp','Mean_timestamp','HRV_MCVNN'], axis=1) # drop start, end and mean timestamp columns present in labelled Feature Data\n",
        "# 'HRV_MCVNN' column consist of infinity values which causes err while modeling so dropping that column as well"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq3-wiT49X-j",
        "outputId": "1cdb88ff-ac18-42ae-b350-6a0275b1024d"
      },
      "source": [
        "DataFrame.subject_ID.value_counts()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "966     89\n",
              "793     89\n",
              "1056    71\n",
              "942     66\n",
              "984     61\n",
              "961     61\n",
              "946     57\n",
              "963     53\n",
              "962     52\n",
              "1058    49\n",
              "820     42\n",
              "877     29\n",
              "937      4\n",
              "Name: subject_ID, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC50xZDx9c1a"
      },
      "source": [
        "from numpy import interp\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate, LeaveOneGroupOut, StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve,auc, roc_auc_score, make_scorer\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLzYe0US9nnb"
      },
      "source": [
        "rf = RandomForestClassifier(random_state = 42) # random forest classifier as rf \n",
        "knn = KNeighborsClassifier() # K-Nearest Neighbor classifier as knn"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03FGQMR69p6l"
      },
      "source": [
        "def Training(empty_exp_list,Data,KFold:bool):\n",
        "  \n",
        "  '''\n",
        "  --- We are carrying out three different Binary classification experiments for each of two event periods (Baseline vs Speech, Speech vs Food Selection, Food selec vs Baseline).\n",
        "      So that we will be able to discriminate the behaviour of each of the event when compared with other event.\n",
        "  --- Implementing K-fold cross val and Leave One Group Out cross val using rf, knn algorithms. \n",
        "  --- This function takes HRV feature data as input along with KFold Boolean value when True computes K-fold Cross val and when False computes LOGO cross val\n",
        "  --- In the LOGO Cross val we are computing the mean scores of all the subjects and also scores while subject is left out.\n",
        "  --- We are getting accuracy score and area under the curve score of each experiment with above mentioned models as dictionary key value pairs.\n",
        "  '''\n",
        "\n",
        "  # split data with relevance to event periods in each experiment and convert event column to category type\n",
        "  VRBaseline_Speech = DataFrame.loc[DataFrame['Event'].isin(['VR baseline','Speech Emotion'])]\n",
        "  VRBaseline_Speech[\"Event\"] = VRBaseline_Speech[\"Event\"].astype('category').cat.codes\n",
        "  VRBaseline_FoodSelec = DataFrame.loc[DataFrame['Event'].isin(['VR baseline','Food Selection'])]\n",
        "  VRBaseline_FoodSelec[\"Event\"] = VRBaseline_FoodSelec[\"Event\"].astype('category').cat.codes\n",
        "  Speech_FoodSelec = DataFrame.loc[DataFrame['Event'].isin(['Food Selection','Speech Emotion'])]\n",
        "  Speech_FoodSelec[\"Event\"] = Speech_FoodSelec[\"Event\"].astype('category').cat.codes\n",
        "\n",
        "  # Giving keys while each experiment dataframe are values\n",
        "  dict_exp = {'VR baseline vs Speech':VRBaseline_Speech,\n",
        "            'VR baseline vs VR FoodSelec':VRBaseline_FoodSelec,\n",
        "            'Speech vs VR FoodSelec':Speech_FoodSelec}\n",
        "\n",
        "  for experiment, data in dict_exp.items():\n",
        "    \n",
        "    # result dictionary with experiment name as key value pair\n",
        "    result_dict = {'experiment':experiment}\n",
        "    \n",
        "    # while KFold is True(input), we are asking this function to carry out K-fold Cross Val\n",
        "    if KFold == True:\n",
        "      data = data.drop(['subject_ID'], axis=1) # Drop subject_ID column as K-fold does not handle subject wise analysis\n",
        "\n",
        "      feature = np.array(data.iloc[:,:-1]) # feature columns\n",
        "      target = np.array(data.iloc[:,-1]) # target column\n",
        "\n",
        "      cv = StratifiedKFold(n_splits=13) # Stratified splitting of data with 13 folds\n",
        "      \n",
        "      # empty lists for accuarcy and auc scores for each algorithm\n",
        "      accuracy_rf=[]\n",
        "      accuracy_knn=[]\n",
        "      auc_rf=[]\n",
        "      auc_knn=[]\n",
        "      \n",
        "      # Loop to carry out predictions on each of the 13 folds and append scores to empty lists algorithm wise and mean is computed as key value apir to result_dict\n",
        "      for train, test in cv.split(feature,target):\n",
        "        subject_ID = None # Drop subject_ID column as K-fold does not handle subject wise analysis\n",
        "\n",
        "        # fitiing both the models\n",
        "        rf_cv=rf.fit(feature[train],target[train])\n",
        "        knn_cv=knn.fit(feature[train],target[train])\n",
        "        \n",
        "        # predicting and finding scores of rf algorithm\n",
        "        y_pred_rf = rf_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_rf)\n",
        "        auc_RF = auc(fpr,tpr)\n",
        "        accuracy_RF = accuracy_score(target[test],y_pred_rf)\n",
        "\n",
        "        # append scores to empty lists\n",
        "        accuracy_rf.append(accuracy_RF)\n",
        "        auc_rf.append(auc_RF)\n",
        "        \n",
        "        # predicting and finding scores of knn algorithm\n",
        "        y_pred_knn = knn_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_knn)\n",
        "        auc_KNN = auc(fpr,tpr)\n",
        "        accuracy_KNN = accuracy_score(target[test],y_pred_knn)\n",
        "\n",
        "        # append scores to empty lists\n",
        "        accuracy_knn.append(accuracy_KNN)\n",
        "        auc_knn.append(auc_KNN)\n",
        "\n",
        "    else: # Where KFold (input) False and the function performs LOGO cross val\n",
        "      subject_id = data['subject_ID'] # taking out the subject column to pass it as list to the LOGO splits\n",
        "      data = data.drop(['subject_ID'], axis=1) # dropping subject_ID\n",
        "\n",
        "      feature = np.array(data.iloc[:,:-1]) # feature columns\n",
        "      target = np.array(data.iloc[:,-1]) # target column\n",
        "      groups = np.array(subject_id)\n",
        "    \n",
        "      logo = LeaveOneGroupOut() # group wise splitting of train and test data\n",
        "      logo.get_n_splits(feature, target, groups)\n",
        "      logo.get_n_splits(groups=groups)\n",
        "      \n",
        "      # empty lists for accuarcy and auc scores for each algorithm\n",
        "      accuracy_rf=[]\n",
        "      accuracy_knn=[]\n",
        "      auc_rf=[]\n",
        "      auc_knn=[]\n",
        "\n",
        "      subject_ID_avg = {}\n",
        "      \n",
        "      # Loop to carry out predictions on each of the 13 folds and append scores to empty lists algorithm wise and mean is computed as key value apir to result_dict\n",
        "      # Subject wise predictions are appended to subject_ID_avg\n",
        "      for train,test in logo.split(feature,target,groups):\n",
        "        \n",
        "        rf_cv=rf.fit(feature[train],target[train])\n",
        "        knn_cv=knn.fit(feature[train],target[train])\n",
        "\n",
        "        y_pred_rf = rf_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_rf)\n",
        "        auc_RF = auc(fpr,tpr)\n",
        "        accuracy_RF = accuracy_score(target[test],y_pred_rf)\n",
        "\n",
        "        accuracy_rf.append(accuracy_RF)\n",
        "        auc_rf.append(auc_RF)\n",
        "\n",
        "        y_pred_knn = knn_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_knn)\n",
        "        auc_KNN = auc(fpr,tpr)\n",
        "        accuracy_KNN = accuracy_score(target[test],y_pred_knn)\n",
        "        \n",
        "        accuracy_knn.append(accuracy_KNN)\n",
        "        auc_knn.append(auc_KNN)\n",
        "\n",
        "        subject_ID = groups[test][0]\n",
        "        subject_ID_avg[str(subject_ID)] = {'accuracy_rf':accuracy_RF,\n",
        "                                          'auc_rf':auc_RF,\n",
        "                                          'accuracy_knn':accuracy_KNN,\n",
        "                                          'auc_knn':auc_KNN}\n",
        "    \n",
        "    if KFold == True:\n",
        "      pass\n",
        "    else:\n",
        "      result_dict['subject_ID'] = subject_ID_avg\n",
        "\n",
        "    result_dict['accuracy_score_testing'] = {'rf_cv':np.mean(accuracy_rf),\n",
        "                                              'knn_cv':np.mean(accuracy_knn)}\n",
        "\n",
        "    result_dict['auc_score_testing'] = {'rf_cv':np.mean(auc_rf),\n",
        "                                              'knn_cv':np.mean(auc_knn)}\n",
        "\n",
        "    empty_exp_list.append(result_dict)\n",
        "\n",
        "  return empty_exp_list"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxScEQF-BaM"
      },
      "source": [
        "KFoldCV_results = []\n",
        "KFoldCV_results = Training(KFoldCV_results,DataFrame, KFold = True) \n",
        "# Predictions while KFold is Ture which means asking to run Kfold cross val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIKJAaE4_wrW"
      },
      "source": [
        "KFoldCV = pd.DataFrame() # empty dataframe\n",
        "# loop to append each of the index from Training functions to a new df and append that df to above empty dataframe\n",
        "for index in KFoldCV_results: \n",
        "  df = pd.DataFrame(index)\n",
        "  KFoldCV = KFoldCV.append(df)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522IerCiCSOI"
      },
      "source": [
        "KFoldCV.to_csv('/content/gdrive/My Drive/Food_VR/Food_VR/ECG_Processing/ECG_Predictions/HRV_predictions_K-FoldCV.csv', index = True) # save it as csv"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XLOT6PiAGMy"
      },
      "source": [
        "LOGO_results = []\n",
        "LOGO_results = Training(LOGO_results,DataFrame, KFold = False)\n",
        "# Predictions while KFold is False which means asking to run LOGO cross val resulting in Mean of LOGO cross val and subject wise predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClGB9pFlARFf"
      },
      "source": [
        "# code in below cells is to separate mean LOGO predictions and subject wise predictions\n",
        "LOGO = pd.DataFrame()\n",
        "for index in LOGO_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  LOGO = LOGO.append(df)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy0Z6X5VAYtl"
      },
      "source": [
        "list_index = LOGO.index\n",
        "indices = []\n",
        "for index in range(0,len(list_index)):\n",
        "  try:\n",
        "    element = ast.literal_eval(list_index[index])\n",
        "  except ValueError:\n",
        "    indices.append(list_index[index])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0j7o2r-AbRx"
      },
      "source": [
        "Mean_LOGO = pd.DataFrame(LOGO.loc[indices])\n",
        "Mean_LOGO = Mean_LOGO.drop_duplicates()\n",
        "Mean_LOGO = Mean_LOGO.drop(columns = ['subject_ID'])\n",
        "Mean_LOGO.to_csv(\"/content/gdrive/My Drive/Food_VR/Food_VR/ECG_Processing/ECG_Predictions/HRV_predictions_MeanLOGO.csv\", index = True) # LOGO Mean Predictions"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrOtgyaKAjp1"
      },
      "source": [
        "Subject_LOGO = LOGO.drop(indices).drop(columns = ['accuracy_score_testing','auc_score_testing'])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yerwq4JlB6V9"
      },
      "source": [
        "list_=[]\n",
        "for w in range(0,len(Subject_LOGO)):\n",
        "  index = Subject_LOGO.index\n",
        "  list_.append(index[w])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M8owsc5B8eg"
      },
      "source": [
        "dummy_df = pd.DataFrame()\n",
        "for one in range(0,len(Subject_LOGO)):\n",
        "  Dict = Subject_LOGO['subject_ID'][one]\n",
        "  df = pd.DataFrame(Dict,index=[list_[one]])\n",
        "  exp = Subject_LOGO['experiment'][one]\n",
        "  df['experiment'] = exp\n",
        "  dummy_df = dummy_df.append(df)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f87a0CYsB-2S"
      },
      "source": [
        "dummy_df.to_csv(\"/content/gdrive/My Drive/Food_VR/Food_VR/ECG_Processing/ECG_Predictions/HRV_predictions_SubjectWise_LOGO.csv\", index = True) # LOGO subject wise predictions"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W48Q7XhN7PIS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}