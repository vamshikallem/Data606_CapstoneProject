{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSR_Predictions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bTGTfa3d5i1",
        "outputId": "20420c8b-bdc8-49a5-947a-8c89d2d6222b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq-TlDHfq7pv",
        "outputId": "ceac441c-7cae-469b-de5d-97fd990f015a"
      },
      "source": [
        "pip install neurokit2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neurokit2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/4a/d2a9502942cb60e61c9ba9772c04ebd0a945fe248ed42cb520334da582b2/neurokit2-0.1.1-py2.py3-none-any.whl (990kB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 16.3MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 14.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 450kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 460kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 471kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 481kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 491kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 501kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 512kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 522kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 532kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 542kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 552kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 563kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 573kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 583kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 593kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 604kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 614kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 624kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 634kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 645kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 655kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 665kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 675kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 686kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 696kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 706kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 716kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 727kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 737kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 747kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 757kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 901kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 911kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 921kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 931kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 942kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 952kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 962kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 972kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 983kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->neurokit2) (1.15.0)\n",
            "Installing collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8kMlRSFq7wL",
        "outputId": "2c313de7-8f2e-40d2-feec-a447b704d47e"
      },
      "source": [
        "pip install biosppy==0.6.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biosppy==0.6.1 in /usr/local/lib/python3.7/dist-packages (0.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.4.1)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (0.22.2.post1)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (0.21.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->biosppy==0.6.1) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrHrb2XBq70p",
        "outputId": "3eda5991-20e9-4d8f-9bf9-9dec48064006"
      },
      "source": [
        "pip install mne"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f7/2bf5de3fad42b66d00ee27539bc3be0260b4e66fdecc12f740cdf2daf2e7/mne-0.23.0-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.23.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSAe0r6vwfDc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time \n",
        "import re\n",
        "import csv\n",
        "import scipy.io\n",
        "import biosppy\n",
        "import mne\n",
        "import neurokit2 as nk\n",
        "import ast\n",
        "import os\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import glob\n",
        "from scipy.stats import zscore, norm\n",
        "from neurokit2 import eda_phasic\n",
        "from scipy.stats import linregress\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4SggKmeo1Ul"
      },
      "source": [
        "# this function is to convert the TimeStamp column (first column) from Unix Epoch time to standard datetime format\n",
        "def TimeStamp_Conversion(ts):\n",
        "  \"\"\"\n",
        "  we have a unix epoch time in milliseconds i.e, a string with a length of 13 charcters example:1.5789360034388428E12\n",
        "  \n",
        "  parameters:\n",
        "  -----\n",
        "  ts = Epoch timesatmp in milliseconds.\n",
        "\n",
        "  Returns:\n",
        "  -----\n",
        "  Std_Unix = standard epoch timestamp in seconds.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  float_Input = float(ts) # converting the string-type(1.5789360034388428E12) Unix Epoch to float-type(1578936003.4388428).\n",
        "\n",
        "  # float input is divided by 1000 to convert the Unix epoch in milliseconds to seconds \n",
        "  Std_Unix = float_Input/1000\n",
        "\n",
        "  datetime_Input = datetime.fromtimestamp(Std_Unix) \n",
        "  # datetime.fromtimestamp converts the unix epoch in seconds to datetime returns example:datetime.datetime(2020, 1, 13, 17, 20, 3, 438843)\n",
        "\n",
        "  return Std_Unix"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcAkYUUXo1f8"
      },
      "source": [
        "def column_formatting(Timestamp_DF):\n",
        "  \"\"\"\n",
        "  Column names of Timestamp annotation excel have column index attached to column name as we only need column name we are parsing column names.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  Timestamp_DF = Input the DF after reading the timestamp annotationexcel file to  get list of column names['A1- ECG baseline start','B1- ECG baseline end',.....].\n",
        "\n",
        "  Returns:\n",
        "  -----\n",
        "  Parsed_ColumnNames = list of parsed column names. ['ECG baseline start','ECG baseline end',....]\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Parsed_colnames = ['Subject_ID'] ## Column with Participant ID is not named, so declaring first column as Subject_ID to an empty list\n",
        "\n",
        "  for index in range(1,len(Timestamp_DF.columns)): ## Looping through the list of timestamp annotation columns list\n",
        "    column = Timestamp_DF.columns[index][4:].lstrip() ## Drop first 3 indices of each column and strip space(\" \") if present as left most\n",
        "    Parsed_colnames.append(column) ## appending each column name after parsing\n",
        "\n",
        "  return Parsed_colnames ## returns list fo parsed col names"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4gviV0mo1i_"
      },
      "source": [
        "def Annotation_timestamp(timestamp_path, sheet_name):\n",
        "  \"\"\"\n",
        "  This function is to change the column names of timestamp annotations table and convert timestamps from milliseconds to standart epoch format of seconds.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  timestamp_path = path to the directory of file location\n",
        "  sheet_name =  there are two sheets present in the file, we work on file named D.\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  VR_Timestamps_D = Clean dataframe of timestamp annotations table.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  VR_TimeStamps_D = pd.read_excel(Timestamp_path, sheet_name) ## read timestamp annotation file\n",
        "  Parsed_colnames = column_formatting(VR_TimeStamps_D) ## using the column_formatting function defined earlier parse columns\n",
        "  VR_TimeStamps_D.columns = Parsed_colnames ## Change colnames of Dataframe using the parsed list of col names\n",
        "  \n",
        "  ## As timestamp is in string format and in milli seconds iterating through each column to change the timestamp to standard epoch format.\n",
        "  for col in VR_TimeStamps_D.columns: \n",
        "    ## Using Timestamp_Conversion function and lambda fucntion to map the function to each row of the column.\n",
        "    if col == 'Subject_ID':\n",
        "      pass\n",
        "    else:\n",
        "      VR_TimeStamps_D[col] = VR_TimeStamps_D[col].map(lambda instance: TimeStamp_Conversion(instance)) \n",
        "\n",
        "  return VR_TimeStamps_D"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngeZSLuxpAm-"
      },
      "source": [
        "def Shimmers_csv2DF(path,filename):\n",
        "  \"\"\"\n",
        "  This function is to read Shimmer data files and create a dataframe from tidy shimmers csv tables.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  path = path to directory of shimmers file folder.\n",
        "\n",
        "  filename = name of the file to be loaded.\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  Dataframe = organized and structured Shimmers Data.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  with open(path + '/' + filename, 'r',) as file: # read the file\n",
        "    reader = csv.reader(file)\n",
        "\n",
        "    lists_eachrow = []\n",
        "    for row in reader:\n",
        "      lists_eachrow.append(row) # append each row in reader to a list\n",
        "\n",
        "  del lists_eachrow[0] # del first row of list as it is only about \\t delimiter used\n",
        "\n",
        "  newlists = [] \n",
        "  # loop through the list of lists and split columnar values using the delimiter \n",
        "  for list_row in lists_eachrow:\n",
        "    for row in list_row:\n",
        "      newlists.append(list(row.split('\\t')))\n",
        "  # Extract subjectID from the file name for future use\n",
        "  filename_parse = filename.replace(\"_\", \" \")\n",
        "  Participant_ID = ast.literal_eval(re.findall(r'\\b\\d+\\b', filename_parse)[0])\n",
        "  \n",
        "  # create dataframe from the list of columnar values \n",
        "  Dataframe = pd.DataFrame(newlists, columns = newlists[0])\n",
        "  Dataframe = Dataframe.drop([0,1]) # drop columns 1 and 2 as are column names and units which we already have for new dataframe.\n",
        "  Dataframe.reset_index(drop=True, inplace=True) # reset index\n",
        "\n",
        "  return Dataframe, Participant_ID "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzEhflQwpAp-"
      },
      "source": [
        "def Sliding_Window_GSRFeatureExt(GSR_16Hz,Participant_ID, start_window, overlap, window_length):\n",
        "  \"\"\"\n",
        "  This function is to create a dataframe consisting of statistical features extracted using sliding window technique on Phasic and Tonic components of GSR signal.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  GSR_16Hz = This the Dataframe consisting of Phasic and Tonic components of GSR signal along with epoch timestamp of each instance.\n",
        "\n",
        "  Participant_ID = Subject ID extracted in Shimmers_csv2DF function from the file name.\n",
        "  \n",
        "  start_window = 0, starting index of the sliding window\n",
        "  \n",
        "  overlap = 50, sliding window tech. with 50 overlap\n",
        "  \n",
        "  window_length = 100, length of each window (100 rows at once)\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  Dataframe consisting of statistical features like Mean Phasic&Tonic components, std Phasic&Tonic components, Count and Mean of Phasic&Tonic Peaks, \n",
        "  Min and Max, Slope, AUC of Phasic&Tonic components for each window of length 100 and overlap of 50.\n",
        "\n",
        " \"\"\"\n",
        "  # creating an array of phasic, tonic and timestamps from the GSR_16Hz dataframe, will be easy to calculate statistical features of numpy array.\n",
        "  phasic  = np.array(GSR_16Hz['phasic']) \n",
        "  tonic  = np.array(GSR_16Hz['tonic'])\n",
        "  timestamp = np.array(GSR_16Hz['TimeStamp'])\n",
        "  dummy = list(range(0,len(GSR_16Hz))) # this is to use as secondary axis to calculate slope.\n",
        "\n",
        "  end_window = window_length\n",
        "  Start_timestamp = []\n",
        "  Mean_timestamp = []\n",
        "  end_timestamp = []\n",
        "  Mean_phasic = []\n",
        "  Mean_tonic = []\n",
        "  Std_phasic = []\n",
        "  Std_tonic = []\n",
        "  CountPeak_phasic = []\n",
        "  CountPeak_tonic = []\n",
        "  MeanPeak_phasic = []\n",
        "  MeanPeak_tonic = []\n",
        "  Min_phasic = []\n",
        "  Max_phasic = []\n",
        "  Min_tonic = []\n",
        "  Max_tonic = []\n",
        "  Slope_tonic = []\n",
        "  Slope_phasic = []\n",
        "  AUC_tonic = []\n",
        "  AUC_phasic = []\n",
        "  while abs(end_window-window_length) <= len(GSR_16Hz): # loop conditon to carry out windowing and calculate features till the end \n",
        "\n",
        "    # creating start, mean and end timestamps helps to  make sure that each window completely falls in annotation period\n",
        "    Start_timestamp.append(timestamp[start_window]) \n",
        "    Mean_timestamp.append(np.mean(timestamp[start_window:end_window]))\n",
        "    try:\n",
        "      end_timestamp.append(timestamp[end_window])\n",
        "    except IndexError: \n",
        "      # index of last window might exceed the last timestamp of our data, if this happens it will use last timestamp of data as end timsatmp of last window\n",
        "      end_timestamp.append(timestamp[-1])\n",
        "\n",
        "    # using numpy to compute statistucal features like mean, std, count\n",
        "    Mean_phasic.append(np.mean(phasic[start_window:end_window]))\n",
        "    Mean_tonic.append(np.mean(tonic[start_window:end_window]))\n",
        "    Std_phasic.append(np.std(phasic[start_window:end_window]))\n",
        "    Std_tonic.append(np.std(tonic[start_window:end_window]))\n",
        "    # scipy.signal.find_peaks exctracts the peaks present in teh signal\n",
        "    CountPeak_phasic.append(scipy.signal.find_peaks(phasic[start_window:end_window])[0].size)\n",
        "    CountPeak_tonic.append(scipy.signal.find_peaks(tonic[start_window:end_window])[0].size)\n",
        "    MeanPeak_phasic.append(np.mean(scipy.signal.find_peaks(phasic[start_window:end_window])[0]))\n",
        "    MeanPeak_tonic.append(np.mean(scipy.signal.find_peaks(tonic[start_window:end_window])[0]))\n",
        "    Min_phasic.append(min(phasic[start_window:end_window]))\n",
        "    Max_phasic.append(max(phasic[start_window:end_window]))\n",
        "    Min_tonic.append(min(tonic[start_window:end_window]))\n",
        "    Max_tonic.append(max(tonic[start_window:end_window]))\n",
        "    # slope is computed using linregress module\n",
        "    Slope_tonic.append(linregress(dummy[start_window:end_window],tonic[start_window:end_window])[0])\n",
        "    Slope_phasic.append(linregress(dummy[start_window:end_window],phasic[start_window:end_window])[0])\n",
        "    # Area under curve is calculated using sk learn metrics\n",
        "    AUC_tonic.append(metrics.auc(dummy[start_window:end_window],tonic[start_window:end_window]))\n",
        "    AUC_phasic.append(metrics.auc(dummy[start_window:end_window],phasic[start_window:end_window]))\n",
        "      \n",
        "    # to increment start and end for next window\n",
        "    start_window += overlap\n",
        "    end_window += overlap\n",
        "  \n",
        "  # create Data frame from the lists of values appended to each feature.\n",
        "  Feature_DF = pd.DataFrame()\n",
        "  Feature_DF['Start_timestamp'] = Start_timestamp\n",
        "  Feature_DF['Mean_timestamp'] = Mean_timestamp\n",
        "  Feature_DF['End_timestamp'] = end_timestamp\n",
        "  Feature_DF['Mean_phasic'] = Mean_phasic\n",
        "  Feature_DF['Mean_tonic'] = Mean_tonic\n",
        "  Feature_DF['Std_phasic'] = Std_phasic\n",
        "  Feature_DF['Std_tonic'] = Std_tonic\n",
        "  Feature_DF['CountPeak_phasic'] = CountPeak_phasic\n",
        "  Feature_DF['CountPeak_tonic'] = CountPeak_tonic\n",
        "  Feature_DF['MeanPeak_phasic'] = MeanPeak_phasic\n",
        "  Feature_DF['MeanPeak_tonic'] = MeanPeak_tonic\n",
        "  Feature_DF['Min_phasic'] = Min_phasic\n",
        "  Feature_DF['Max_phasic'] = Max_phasic\n",
        "  Feature_DF['Min_tonic'] = Min_tonic\n",
        "  Feature_DF['Max_tonic'] = Max_tonic\n",
        "  Feature_DF['Slope_tonic'] = Slope_tonic\n",
        "  Feature_DF['Slope_phasic'] = Slope_phasic\n",
        "  Feature_DF['AUC_tonic'] = AUC_tonic\n",
        "  Feature_DF['AUC_phasic'] = AUC_phasic\n",
        "  Feature_DF['Subject_ID'] = Participant_ID\n",
        "\n",
        "  return Feature_DF"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4V5VqnIwFpt"
      },
      "source": [
        "def GSR_features(path,Timestamp_path,method = 'cvxEDA',sheet_name = 'D',start_window=0, overlap=50, window_length=100):\n",
        "  \n",
        "  # taking path location of Shimmer file as input and using glob function to find csv and text files in folder and appending each subject's filename to a list\n",
        "  os.chdir(path)\n",
        "  list_csv = glob.glob('*.{}'.format('txt'))\n",
        "  list_csv.extend(glob.glob('*.{}'.format('csv')))\n",
        "  \n",
        "  # Creating an empty DataFrame to append all the features\n",
        "  Data = pd.DataFrame()\n",
        "\n",
        "  # loop to feature extraction on each subject file.\n",
        "  for index in list_csv:\n",
        "    # formatting csv file to create a clean DataFrame and extract Participant ID from file name using Shimmers_csv2DF function\n",
        "    Dataframe, Participant_ID = Shimmers_csv2DF(path,index)\n",
        "\n",
        "    GSR_128Hz = pd.DataFrame(Dataframe, columns = ['Shimmer_89C4_Timestamp_Unix_CAL','Shimmer_89C4_GSR_Skin_Conductance_CAL']) # just using timestamp and GSr signal columns\n",
        "    \n",
        "    # Convert the timestamp column in nanoseconds to milliseconds\n",
        "    GSR_128Hz['Timestamp_s'] = GSR_128Hz['Shimmer_89C4_Timestamp_Unix_CAL'].map(lambda instance: TimeStamp_Conversion(instance))\n",
        "\n",
        "    signal = list(GSR_128Hz['Shimmer_89C4_GSR_Skin_Conductance_CAL']) # list out of GSR signal column\n",
        "    signal = list(map(lambda index : ast.literal_eval(index),signal)) # string type to int\n",
        "\n",
        "    signal = mne.filter.resample(signal, down = 8) # downsample using mne resample by factor of 8\n",
        "    len_downsample = len(signal) \n",
        "    \n",
        "    # taking start and end time of timestamp column to create a sequential timestamp list of downsampled size\n",
        "    start_time = list(GSR_128Hz['Timestamp_s'])[0] \n",
        "    end_time = list(GSR_128Hz['Timestamp_s'])[-1]\n",
        "    duration= end_time-start_time\n",
        "    downsample_freq = len_downsample/duration\n",
        "\n",
        "    list_ts = np.array(np.linspace(start_time,end_time,len_downsample))\n",
        "\n",
        "    # creating new df with downsampled size \n",
        "    GSR_16Hz = pd.DataFrame()\n",
        "    GSR_16Hz['TimeStamp'] = list_ts\n",
        "    GSR_16Hz['GSRC'] = signal\n",
        "    \n",
        "    # Smoothen and normalize the signal\n",
        "    GSR_16Hz['GSRC'] = GSR_16Hz['GSRC'].ewm(span = 16).mean() # carryout estimated weighted moving average to \n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    GSR_16Hz['GSRC'] = min_max_scaler.fit_transform(np.array(GSR_16Hz['GSRC']).reshape(-1, 1)) # Normalize using MinMaxScalar\n",
        " \n",
        "    eda = (GSR_16Hz['GSRC'])\n",
        "\n",
        "    # extracting phasic and tonic components using eda_phasic() function from neurokit.py module\n",
        "    Phasic_Tonic_DF = eda_phasic(eda,downsample_freq, method)\n",
        "\n",
        "    GSR_16Hz['phasic'] = Phasic_Tonic_DF['EDA_Phasic']\n",
        "    GSR_16Hz['tonic'] = Phasic_Tonic_DF['EDA_Tonic']\n",
        "\n",
        "    # Implenting sliding window technique to extract statistical features from phasic and tonic components\n",
        "    Feature_DF = Sliding_Window_GSRFeatureExt(GSR_16Hz,Participant_ID, start_window, overlap, window_length)\n",
        "    \n",
        "    # Input event timestamp path to Annnotation_timstamp function to get a clean event period timestamp\n",
        "    VR_TimeStamps_D = Annotation_timestamp(Timestamp_path, sheet_name)\n",
        "    \n",
        "    # for each subject present in subject list we take out start and end time stamps of each event period \n",
        "    VRBaseline_start = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == Participant_ID,'VR baseline start'].iloc[0]\n",
        "    VRBaseline_end = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == Participant_ID,'VR baseline end'].iloc[0]\n",
        "    Speech_start = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == Participant_ID,'Emotion-induction speech start'].iloc[0]\n",
        "    Speech_end = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == Participant_ID,'Emotion-induction speech end'].iloc[0]\n",
        "    Food_start = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == Participant_ID,'Food selection start'].iloc[0]\n",
        "    Food_end = VR_TimeStamps_D.loc[VR_TimeStamps_D['Subject_ID'] == Participant_ID,'food selection end'].iloc[0]\n",
        "\n",
        "    Feature_DF['Event'] = ''\n",
        "\n",
        "    # using the start and end timestamps of each event period we check whether the start and end timestamps of ecah feature row fall in event start and end, if yes it is labelled as particular event and if not check for other event\n",
        "    # if the feature start and end timestamp does not lie in any of the vents start and end timestamp it results in NAN label \n",
        "    for i in range(0,len(Feature_DF)):\n",
        "      if VRBaseline_start <= Feature_DF['Start_timestamp'][i] <= VRBaseline_end and VRBaseline_start <= Feature_DF['Mean_timestamp'][i] <= VRBaseline_end and VRBaseline_start <= Feature_DF['End_timestamp'][i] <= VRBaseline_end:\n",
        "        Feature_DF['Event'][i] = 'VR Baseline'\n",
        "      elif Speech_start <= Feature_DF['Start_timestamp'][i] <= Speech_end and Speech_start <= Feature_DF['Mean_timestamp'][i] <= Speech_end and Speech_start <= Feature_DF['End_timestamp'][i] <= Speech_end:\n",
        "        Feature_DF['Event'][i] = 'Speech Emotion'\n",
        "      elif Food_start <= Feature_DF['Start_timestamp'][i] <= Food_end and Food_start <= Feature_DF['Mean_timestamp'][i] <= Food_end and Food_start <= Feature_DF['End_timestamp'][i] <= Food_end:\n",
        "        Feature_DF['Event'][i] = 'Food Selection'\n",
        "      else:\n",
        "        Feature_DF.drop(i, inplace = True)\n",
        "\n",
        "    # append features from each window to an empty Dataframe \n",
        "    Data = Data.append(Feature_DF, ignore_index=True)\n",
        "  \n",
        "  return Data # labelled Feature Data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEQPsiepqlLa"
      },
      "source": [
        "path = '/content/gdrive/My Drive/Food_VR/Food_VR/GSR_Processing/GSR_Data'\n",
        "Timestamp_path = '/content/gdrive/My Drive/Food_VR/Food_VR/VR Timestamps for Phase B & D_W&SP20.xlsx'\n",
        "Data = GSR_features(path,Timestamp_path,method = 'cvxEDA',sheet_name = 'D',start_window=0, overlap=50, window_length=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMnstLeTE-WE"
      },
      "source": [
        "Data.to_csv(\"/content/gdrive/My Drive/Food_VR/Food_VR/GSR_Processing/GSR_Features.csv\", index = True) # save features"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyzOivb5HIH4"
      },
      "source": [
        "Data = pd.read_csv(\"/content/gdrive/My Drive/Food_VR/Food_VR/GSR_Processing/GSR_Features.csv\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGGR4m060Ssw"
      },
      "source": [
        "DataFrame = Data.drop(['End_timestamp','Start_timestamp','Mean_timestamp' ], axis=1) # drop timesatmp columns"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZptoKEyRzodj",
        "outputId": "8d9d0f2a-ad3d-4adc-b1d6-a9abe3d912a2"
      },
      "source": [
        "DataFrame.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1462, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQnI7YLO0Z-J"
      },
      "source": [
        "DataFrame = DataFrame.fillna(0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pN1XBxi1UME",
        "outputId": "a9d550f5-6a14-4ff6-8d4c-fbacac444a66"
      },
      "source": [
        "DataFrame.Event.value_counts()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Food Selection    489\n",
              "Speech Emotion    487\n",
              "VR Baseline       486\n",
              "Name: Event, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAgeTrjH1jJQ",
        "outputId": "7396d862-8d0f-4158-9b1d-f897642929b5"
      },
      "source": [
        "DataFrame.Subject_ID.value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "966     171\n",
              "984     121\n",
              "1056    119\n",
              "961     118\n",
              "820     118\n",
              "946     111\n",
              "942     111\n",
              "877     111\n",
              "793     108\n",
              "963     102\n",
              "1058     99\n",
              "962      99\n",
              "937      74\n",
              "Name: Subject_ID, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWDLIB_EXudO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luluwy_la5OB"
      },
      "source": [
        "def Tonic_col(columns):\n",
        "  tonic_col = []\n",
        "  for x in columns:\n",
        "    if 'tonic' in x:\n",
        "      tonic_col.append(x)\n",
        "  return tonic_col"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfSdYYZYYTp"
      },
      "source": [
        "from numpy import interp\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate, LeaveOneGroupOut, StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve,auc, roc_auc_score, make_scorer\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pylab as plt"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlfRIcJmDWGh"
      },
      "source": [
        "rf = RandomForestClassifier(random_state = 42) # random forest classifier as rf \n",
        "logreg = LogisticRegression(solver='lbfgs',penalty='l2',max_iter=500) # Logistic Regression Classifier as logreg\n",
        "knn = KNeighborsClassifier() # K-Nearest Neighbor classifier as knn"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-V9IDgoDTXg"
      },
      "source": [
        "def Training(empty_exp_list,Data,Phasic:bool,Tonic:bool,KFold:bool):\n",
        "\n",
        "  '''\n",
        "  --- We are carrying out three different Binary classification experiments for each of two event periods (Baseline vs Speech, Speech vs Food Selection, Food selec vs Baseline).\n",
        "      So that we will be able to discriminate the behaviour of each of the event when compared with other event.\n",
        "  --- Implementing K-fold cross val and Leave One Group Out cross val using rf, logreg, knn algorithms. \n",
        "  --- This function takes GSR feature data as input along with phasic, tonic, KFold Boolean value when True computes K-fold Cross val and when False computes LOGO cross val\n",
        "  --- In the LOGO Cross val we are computing the mean scores of all the subjects and also scores while one subject is left out.\n",
        "  --- We are getting accuracy score and area under the curve score of each experiment with above mentioned models as dictionary key value pairs.\n",
        "  '''\n",
        "  \n",
        "  if Phasic == True: # if Phasic input is True, function carry out predictions on phasic data\n",
        "    col_list = Data.columns\n",
        "    tonic_col = Tonic_col(col_list) # Tonic_col function to extract tonic columns\n",
        "    DataFrame = Data.drop(tonic_col,axis =1)\n",
        "  elif Tonic == True: # if Tonic input is True, function carry out predictions on tonic data\n",
        "    col_list = Data.columns\n",
        "    tonic_col = Tonic_col(col_list) # Tonic_col function to extract tonic columns\n",
        "    tonic_col.append('Event') # append Event \n",
        "    tonic_col.append('Subject_ID') # append Subject_ID\n",
        "    DataFrame = Data[tonic_col]\n",
        "  else: # if None of Phasic or Tonic input is True, function carries out predictions on both the phasic and tonic data\n",
        "    DataFrame = Data\n",
        "  \n",
        "  # split data with relevance to event periods in each experiment and convert event column to category type\n",
        "  VRBaseline_Speech = DataFrame.loc[DataFrame['Event'].isin(['VR Baseline','Speech Emotion'])]\n",
        "  VRBaseline_Speech[\"Event\"] = VRBaseline_Speech[\"Event\"].astype('category').cat.codes\n",
        "  VRBaseline_FoodSelec = DataFrame.loc[DataFrame['Event'].isin(['VR Baseline','Food Selection'])]\n",
        "  VRBaseline_FoodSelec[\"Event\"] = VRBaseline_FoodSelec[\"Event\"].astype('category').cat.codes\n",
        "  Speech_FoodSelec = DataFrame.loc[DataFrame['Event'].isin(['Food Selection','Speech Emotion'])]\n",
        "  Speech_FoodSelec[\"Event\"] = Speech_FoodSelec[\"Event\"].astype('category').cat.codes\n",
        "\n",
        "  # Giving keys while each experiment dataframe are values  \n",
        "  dict_exp = {'VR Baseline vs Speech':VRBaseline_Speech,\n",
        "            'VR Baseline vs VR FoodSelec':VRBaseline_FoodSelec,\n",
        "            'Speech vs VR FoodSelec':Speech_FoodSelec}\n",
        "\n",
        "  for experiment, data in dict_exp.items():\n",
        "    \n",
        "    # result dictionary with experiment name as key value pair\n",
        "    result_dict = {'experiment':experiment}\n",
        "    \n",
        "    # while KFold is True(input), we are asking this function to carry out K-fold Cross Val\n",
        "    if KFold == True:\n",
        "\n",
        "      # Drop subject_ID column as K-fold does not handle subject wise analysis\n",
        "      data = data.drop(['Subject_ID'], axis=1)\n",
        "\n",
        "      feature = np.array(data.iloc[:,:-1]) # feature columns\n",
        "      target = np.array(data.iloc[:,-1]) # target column\n",
        "\n",
        "      cv = StratifiedKFold(n_splits=13) # Stratified splitting of data with 13 folds\n",
        "\n",
        "      # empty lists for accuarcy and auc scores for each algorithm\n",
        "      accuracy_rf=[]\n",
        "      accuracy_logreg=[]\n",
        "      accuracy_knn=[]\n",
        "      auc_rf=[]\n",
        "      auc_logreg=[]\n",
        "      auc_knn=[]\n",
        "      \n",
        "      # Loop to carry out predictions on each of the 13 folds and append scores to empty lists algorithm wise and mean is computed as key value apir to result_dict\n",
        "      for train, test in cv.split(feature,target):\n",
        "        subject_ID = None\n",
        "\n",
        "        # fitiing both the models\n",
        "        rf_cv=rf.fit(feature[train],target[train])\n",
        "        logreg_cv=logreg.fit(feature[train],target[train])\n",
        "        knn_cv=knn.fit(feature[train],target[train])\n",
        "\n",
        "        # predicting and finding scores of rf algorithm\n",
        "        y_pred_rf = rf_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_rf)\n",
        "        auc_RF = auc(fpr,tpr)\n",
        "        accuracy_RF = accuracy_score(target[test],y_pred_rf)\n",
        "\n",
        "        accuracy_rf.append(accuracy_RF)\n",
        "        auc_rf.append(auc_RF)\n",
        "\n",
        "        # predicting and finding scores of logreg algorithm\n",
        "        y_pred_logreg = logreg_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_logreg)\n",
        "        auc_LR = auc(fpr,tpr)\n",
        "        accuracy_LR = accuracy_score(target[test],y_pred_logreg)\n",
        "\n",
        "        accuracy_logreg.append(accuracy_LR)\n",
        "        auc_logreg.append(auc_LR)\n",
        "        \n",
        "        # predicting and finding scores of knn algorithm\n",
        "        y_pred_knn = knn_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_logreg)\n",
        "        auc_KNN = auc(fpr,tpr)\n",
        "        accuracy_KNN = accuracy_score(target[test],y_pred_knn)\n",
        "\n",
        "        accuracy_knn.append(accuracy_KNN)\n",
        "        auc_knn.append(auc_KNN)\n",
        "\n",
        "    else: # Where KFold (input) False and the function performs LOGO cross val\n",
        "      subject_id = data['Subject_ID'] # taking out the subject column to pass it as list to the LOGO splits\n",
        "      data = data.drop(['Subject_ID'], axis=1)\n",
        "\n",
        "      feature = np.array(data.iloc[:,:-1]) # feature columns\n",
        "      target = np.array(data.iloc[:,-1]) # target column\n",
        "      groups = np.array(subject_id)\n",
        "    \n",
        "      logo = LeaveOneGroupOut() # group wise splitting of train and test data\n",
        "      logo.get_n_splits(feature, target, groups)\n",
        "      logo.get_n_splits(groups=groups)\n",
        "\n",
        "      # empty lists for accuarcy and auc scores for each algorithm\n",
        "      accuracy_rf=[]\n",
        "      accuracy_logreg=[]\n",
        "      accuracy_knn=[]\n",
        "      auc_rf=[]\n",
        "      auc_logreg=[]\n",
        "      auc_knn=[]\n",
        "\n",
        "      subject_ID_avg = {}\n",
        "\n",
        "      # Loop to carry out predictions on each of the 13 folds and append scores to empty lists algorithm wise and mean is computed as key value apir to result_dict\n",
        "      # Subject wise predictions are appended to subject_ID_avg\n",
        "      for train,test in logo.split(feature,target,groups):\n",
        "        \n",
        "        rf_cv=rf.fit(feature[train],target[train])\n",
        "        logreg_cv=logreg.fit(feature[train],target[train])\n",
        "        knn_cv=knn.fit(feature[train],target[train])\n",
        "\n",
        "        y_pred_rf = rf_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_rf)\n",
        "        auc_RF = auc(fpr,tpr)\n",
        "        accuracy_RF = accuracy_score(target[test],y_pred_rf)\n",
        "\n",
        "        accuracy_rf.append(accuracy_RF)\n",
        "        auc_rf.append(auc_RF)\n",
        "\n",
        "        y_pred_logreg = logreg_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_logreg)\n",
        "        auc_LR = auc(fpr,tpr)\n",
        "        accuracy_LR = accuracy_score(target[test],y_pred_logreg)\n",
        "\n",
        "        accuracy_logreg.append(accuracy_LR)\n",
        "        auc_logreg.append(auc_LR)\n",
        "\n",
        "        y_pred_knn = knn_cv.predict(feature[test])\n",
        "        fpr, tpr, threshhold = roc_curve(target[test],y_pred_knn)\n",
        "        auc_KNN = auc(fpr,tpr)\n",
        "        accuracy_KNN = accuracy_score(target[test],y_pred_knn)\n",
        "        \n",
        "        accuracy_knn.append(accuracy_KNN)\n",
        "        auc_knn.append(auc_KNN)\n",
        "\n",
        "        subject_ID = groups[test][0]\n",
        "        subject_ID_avg[str(subject_ID)] = {'accuracy_rf':accuracy_RF,\n",
        "                                          'auc_rf':auc_RF, \n",
        "                                          'accuracy_logreg':accuracy_LR,\n",
        "                                          'auc_logreg':auc_LR,\n",
        "                                          'accuracy_knn':accuracy_KNN,\n",
        "                                          'auc_knn':auc_KNN}\n",
        "    \n",
        "    if KFold == True:\n",
        "      pass\n",
        "    else:\n",
        "      result_dict['Subject_ID'] = subject_ID_avg\n",
        "\n",
        "    result_dict['accuracy_score_testing'] = {'rf_cv':np.mean(accuracy_rf),\n",
        "                                              'logreg_cv':np.mean(accuracy_logreg),\n",
        "                                              'knn_cv':np.mean(accuracy_knn)}\n",
        "\n",
        "    result_dict['auc_score_testing'] = {'rf_cv':np.mean(auc_rf),\n",
        "                                              'logreg_cv':np.mean(auc_logreg),\n",
        "                                              'knn_cv':np.mean(auc_knn)}\n",
        "\n",
        "    empty_exp_list.append(result_dict)\n",
        "\n",
        "  return empty_exp_list"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxO-Y7rCXW0z"
      },
      "source": [
        "KFoldCV_results = []\n",
        "KFoldCV_results = Training(KFoldCV_results,DataFrame,Phasic = False, Tonic = False, KFold = True)\n",
        "# Predictions while KFold is Ture which means asking to run Kfold cross val and on both phasic and tonic columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-8bo8CouSUK"
      },
      "source": [
        "KFoldCV = pd.DataFrame()# empty dataframe\n",
        "# loop to append each of the index from Training functions to a new df and append that df to above empty dataframe\n",
        "for index in KFoldCV_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  KFoldCV = KFoldCV.append(df)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "mDxzA7cJe1jQ",
        "outputId": "7991d775-f04e-4fc2-d90a-5d20ba18be83"
      },
      "source": [
        "KFoldCV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>accuracy_score_testing</th>\n",
              "      <th>auc_score_testing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>rf_cv</th>\n",
              "      <td>VR Baseline vs Speech</td>\n",
              "      <td>0.889785</td>\n",
              "      <td>0.890387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>logreg_cv</th>\n",
              "      <td>VR Baseline vs Speech</td>\n",
              "      <td>0.814082</td>\n",
              "      <td>0.815215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>knn_cv</th>\n",
              "      <td>VR Baseline vs Speech</td>\n",
              "      <td>0.849702</td>\n",
              "      <td>0.815215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rf_cv</th>\n",
              "      <td>VR Baseline vs VR FoodSelec</td>\n",
              "      <td>0.821538</td>\n",
              "      <td>0.822218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>logreg_cv</th>\n",
              "      <td>VR Baseline vs VR FoodSelec</td>\n",
              "      <td>0.704615</td>\n",
              "      <td>0.706177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>knn_cv</th>\n",
              "      <td>VR Baseline vs VR FoodSelec</td>\n",
              "      <td>0.715897</td>\n",
              "      <td>0.706177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rf_cv</th>\n",
              "      <td>Speech vs VR FoodSelec</td>\n",
              "      <td>0.485762</td>\n",
              "      <td>0.484462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>logreg_cv</th>\n",
              "      <td>Speech vs VR FoodSelec</td>\n",
              "      <td>0.561538</td>\n",
              "      <td>0.559935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>knn_cv</th>\n",
              "      <td>Speech vs VR FoodSelec</td>\n",
              "      <td>0.497976</td>\n",
              "      <td>0.559935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            experiment  ...  auc_score_testing\n",
              "rf_cv            VR Baseline vs Speech  ...           0.890387\n",
              "logreg_cv        VR Baseline vs Speech  ...           0.815215\n",
              "knn_cv           VR Baseline vs Speech  ...           0.815215\n",
              "rf_cv      VR Baseline vs VR FoodSelec  ...           0.822218\n",
              "logreg_cv  VR Baseline vs VR FoodSelec  ...           0.706177\n",
              "knn_cv     VR Baseline vs VR FoodSelec  ...           0.706177\n",
              "rf_cv           Speech vs VR FoodSelec  ...           0.484462\n",
              "logreg_cv       Speech vs VR FoodSelec  ...           0.559935\n",
              "knn_cv          Speech vs VR FoodSelec  ...           0.559935\n",
              "\n",
              "[9 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmF0fcvuZA5"
      },
      "source": [
        "KFoldCV.to_csv(\"Phasic&Tonic_predictions_K-FoldCV.csv\", index = True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5RGEfxY6NB8"
      },
      "source": [
        "KFoldCV_Phasic_results = []\n",
        "KFoldCV_Phasic_results = Training(KFoldCV_Phasic_results,DataFrame,Phasic = True, Tonic= False, KFold = True)\n",
        "# Predictions while KFold is Ture which means asking to run Kfold cross val and on both phasic columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ZwhvHvro3f"
      },
      "source": [
        "KFoldCV_Phasic = pd.DataFrame()# empty dataframe\n",
        "# loop to append each of the index from Training functions to a new df and append that df to above empty dataframe\n",
        "for index in KFoldCV_Phasic_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  KFoldCV_Phasic = KFoldCV_Phasic.append(df)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAa3ZnH1tR2d"
      },
      "source": [
        "KFoldCV_Phasic.to_csv(\"PhasicFeatures_predictions_K-FoldCV.csv\", index = True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJxwt23MB5F1"
      },
      "source": [
        "KFoldCV_Tonic_results = []\n",
        "KFoldCV_Tonic_results = Training(KFoldCV_Tonic_results,DataFrame,Phasic = False, Tonic= True, KFold = True)\n",
        "# Predictions while KFold is Ture which means asking to run Kfold cross val and on both tonic columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwgoLgH8B5It"
      },
      "source": [
        "KFoldCV_Tonic = pd.DataFrame()# empty dataframe\n",
        "# loop to append each of the index from Training functions to a new df and append that df to above empty dataframe\n",
        "for index in KFoldCV_Tonic_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  KFoldCV_Tonic = KFoldCV_Tonic.append(df)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uElExnBdB5SB"
      },
      "source": [
        "KFoldCV_Tonic.to_csv(\"/content/gdrive/My Drive/Food_VR/Food_VR/GSR_Processing/GSR_Predictions/TonicFeatures_predictions_K-FoldCV.csv\", index = True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf5xJCu8Do8m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciELokuCDX2G"
      },
      "source": [
        "LOGO_results = []\n",
        "LOGO_results = Training(LOGO_results,DataFrame,Phasic = False, Tonic=False, KFold = False)\n",
        "# Predictions while KFold is False which means asking to run LOGO cross val resulting in Mean of LOGO cross val and subject wise predictions on both phasic and tonic data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YksAihR5uwlU"
      },
      "source": [
        "# code in below cells is to separate mean LOGO predictions and subject wise predictions\n",
        "LOGO = pd.DataFrame()\n",
        "for index in LOGO_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  LOGO = LOGO.append(df)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjkjQB1140ON"
      },
      "source": [
        "list_index = LOGO.index\n",
        "indices = []\n",
        "for index in range(0,len(list_index)):\n",
        "  try:\n",
        "    element = ast.literal_eval(list_index[index])\n",
        "  except ValueError:\n",
        "    indices.append(list_index[index])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhGHUTu58-93"
      },
      "source": [
        "Mean_LOGO = pd.DataFrame(LOGO.loc[indices])\n",
        "Mean_LOGO = Mean_LOGO.drop_duplicates()\n",
        "Mean_LOGO = Mean_LOGO.drop(columns = ['Subject_ID'])\n",
        "Mean_LOGO.to_csv(\"Phasic&Tonic_predicitons_MeanLOGO.csv\", index = True) # save Mean LOGO of both Phasic and TOnic "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhMpbGURJQMc"
      },
      "source": [
        "Subject_LOGO = LOGO.drop(indices).drop(columns = ['accuracy_score_testing','auc_score_testing'])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4_4bgIOOk3c"
      },
      "source": [
        "list_=[]\n",
        "for w in range(0,len(Subject_LOGO)):\n",
        "  index = Subject_LOGO.index\n",
        "  list_.append(index[w])\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEAUzRMpLIm4"
      },
      "source": [
        "dummy_df = pd.DataFrame()\n",
        "for one in range(0,len(Subject_LOGO)):\n",
        "  Dict = Subject_LOGO['Subject_ID'][one]\n",
        "  df = pd.DataFrame(Dict,index=[list_[one]])\n",
        "  exp = Subject_LOGO['experiment'][one]\n",
        "  df['experiment'] = exp\n",
        "  dummy_df = dummy_df.append(df)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NlHQST8iDKZ"
      },
      "source": [
        "dummy_df.to_csv(\"Phasic&Tonic_predictions_LOGO.csv\", index = True) # save subjectwise LOGO predictions of both phasic and tonic data"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAHVMV1FZX9t"
      },
      "source": [
        "LOGO_phasic_results = []\n",
        "LOGO_phasic_results = Training(LOGO_phasic_results,DataFrame,Phasic = True, Tonic = False, KFold = False)\n",
        "# Predictions while KFold is False which means asking to run LOGO cross val resulting in Mean of LOGO cross val and subject wise predictions on both phasic data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaKcZGNCZyty"
      },
      "source": [
        "# code in below cells is to separate mean LOGO predictions and subject wise predictions\n",
        "LOGO_phasic = pd.DataFrame()\n",
        "for index in LOGO_phasic_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  LOGO_phasic = LOGO_phasic.append(df)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sfq-8L1QD4N"
      },
      "source": [
        "list_index = LOGO_phasic.index\n",
        "indices = []\n",
        "for index in range(0,len(list_index)):\n",
        "  try:\n",
        "    element = ast.literal_eval(list_index[index])\n",
        "  except ValueError:\n",
        "    indices.append(list_index[index])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtRAhR8AQNnY"
      },
      "source": [
        "Mean_LOGO_phasic = pd.DataFrame(LOGO_phasic.loc[indices])\n",
        "Mean_LOGO_phasic = Mean_LOGO_phasic.drop_duplicates()\n",
        "Mean_LOGO_phasic = Mean_LOGO_phasic.drop(columns = ['Subject_ID'])\n",
        "Mean_LOGO_phasic.to_csv(\"Phasic_predicitons_MeanLOGO.csv\", index = True) # save Mean LOGO of Phasic"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OmDLnuiQk1U"
      },
      "source": [
        "Subject_LOGO = LOGO_phasic.drop(indices).drop(columns = ['accuracy_score_testing','auc_score_testing'])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89yToSPBiola"
      },
      "source": [
        "list_=[]\n",
        "for w in range(0,len(Subject_LOGO)):\n",
        "  index = Subject_LOGO.index\n",
        "  list_.append(index[w])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBF2Ppr9ir_4"
      },
      "source": [
        "dummy_df = pd.DataFrame()\n",
        "for one in range(0,len(Subject_LOGO)):\n",
        "  Dict = Subject_LOGO['Subject_ID'][one]\n",
        "  df = pd.DataFrame(Dict,index=[list_[one]])\n",
        "  exp = Subject_LOGO['experiment'][one]\n",
        "  df['experiment'] = exp\n",
        "  dummy_df = dummy_df.append(df)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-BaXfiTit-a"
      },
      "source": [
        "dummy_df.to_csv(\"Phasic_predictions_LOGO.csv\", index = True)# save subjectwise LOGO predictions of phasic"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jJ2wbM9ixJ2"
      },
      "source": [
        "LOGO_tonic_results = []\n",
        "LOGO_tonic_results = Training(LOGO_tonic_results,DataFrame,Phasic = False, Tonic = True, KFold = False)\n",
        "# Predictions while KFold is False which means asking to run LOGO cross val resulting in Mean of LOGO cross val and subject wise predictions on phasic data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZOwNr10JK3Z"
      },
      "source": [
        "LOGO_tonic = pd.DataFrame()\n",
        "for index in LOGO_tonic_results:\n",
        "  df = pd.DataFrame(index)\n",
        "  LOGO_tonic = LOGO_tonic.append(df)\n",
        "# code in below cells is to separate mean LOGO predictions and subject wise predictions"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p5Vc8YIJXm7"
      },
      "source": [
        "list_index = LOGO_tonic.index\n",
        "indices = []\n",
        "for index in range(0,len(list_index)):\n",
        "  try:\n",
        "    element = ast.literal_eval(list_index[index])\n",
        "  except ValueError:\n",
        "    indices.append(list_index[index])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRqj-T4jJcSa"
      },
      "source": [
        "Mean_LOGO_tonic = pd.DataFrame(LOGO_tonic.loc[indices])\n",
        "Mean_LOGO_tonic = Mean_LOGO_tonic.drop_duplicates()\n",
        "Mean_LOGO_tonic = Mean_LOGO_tonic.drop(columns = ['Subject_ID'])\n",
        "Mean_LOGO_tonic.to_csv(\"Tonic_predicitons_MeanLOGO.csv\", index = True)# save Mean LOGO of Tonic "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSiwCNMfJrfb"
      },
      "source": [
        "Subject_LOGO = LOGO_tonic.drop(indices).drop(columns = ['accuracy_score_testing','auc_score_testing'])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIVVG4mxJxYq"
      },
      "source": [
        "list_=[]\n",
        "for w in range(0,len(Subject_LOGO)):\n",
        "  index = Subject_LOGO.index\n",
        "  list_.append(index[w])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cobd25HrJznr"
      },
      "source": [
        "dummy_df = pd.DataFrame()\n",
        "for one in range(0,len(Subject_LOGO)):\n",
        "  Dict = Subject_LOGO['Subject_ID'][one]\n",
        "  df = pd.DataFrame(Dict,index=[list_[one]])\n",
        "  exp = Subject_LOGO['experiment'][one]\n",
        "  df['experiment'] = exp\n",
        "  dummy_df = dummy_df.append(df)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf-gSN7uJ27g"
      },
      "source": [
        "dummy_df.to_csv(\"Tonic_predictions_LOGO.csv\", index = True)# save subjectwise LOGO predictions of tonic data"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf38hlAeJ7e_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}