{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreProcessing_Functions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_H-6TXhbYO3",
        "outputId": "4b1a09fa-e340-411a-e2bb-529fd8164a46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/My Drive/Food_VR/GSR_Preprocessing/W&S_D_Shimmer_GSR')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "k0Dyu--0AFTb",
        "outputId": "1a6d985e-9929-4f0b-910b-c07cd5f63d95"
      },
      "source": [
        "import preprocessing_functions.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-95797c42f067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing_functions'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r61Vifb-okz",
        "outputId": "fb027c35-7415-4824-8df9-593958f675b0"
      },
      "source": [
        "pip install neurokit2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neurokit2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/4a/d2a9502942cb60e61c9ba9772c04ebd0a945fe248ed42cb520334da582b2/neurokit2-0.1.1-py2.py3-none-any.whl (990kB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 13.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 8.0MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 7.5MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 7.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 450kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 460kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 471kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 481kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 491kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 501kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 512kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 522kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 532kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 542kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 552kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 563kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 573kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 583kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 593kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 604kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 614kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 624kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 634kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 645kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 655kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 665kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 675kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 686kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 696kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 706kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 716kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 727kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 737kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 747kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 757kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 901kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 911kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 921kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 931kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 942kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 952kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 962kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 972kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 983kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurokit2) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurokit2) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurokit2) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurokit2) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->neurokit2) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->neurokit2) (1.15.0)\n",
            "Installing collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mZhXEG1-ono",
        "outputId": "fa02c9db-b0ac-47dd-aef7-9e33932dd6e6"
      },
      "source": [
        "pip install biosppy==0.6.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting biosppy==0.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/60/d09a277f9d31a2fc9190edf7e8a685c4f9b54b5dff487f523b916f441e1a/biosppy-0.6.1-py2.py3-none-any.whl (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.9MB/s \n",
            "\u001b[?25hCollecting shortuuid\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting bidict\n",
            "  Downloading https://files.pythonhosted.org/packages/67/d4/eaf9242722bf991e0955380dd6168020cb15a71cc0d3cc2373f4911b1f1d/bidict-0.21.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from biosppy==0.6.1) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy==0.6.1) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->biosppy==0.6.1) (1.0.1)\n",
            "Installing collected packages: shortuuid, bidict, biosppy\n",
            "Successfully installed bidict-0.21.2 biosppy-0.6.1 shortuuid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwsfNPR3-o1_",
        "outputId": "9371b904-58e5-427e-f74a-ea12d68b426a"
      },
      "source": [
        "pip install mne"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f7/2bf5de3fad42b66d00ee27539bc3be0260b4e66fdecc12f740cdf2daf2e7/mne-0.23.0-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.23.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfolXr5lbYEG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time \n",
        "import re\n",
        "import csv\n",
        "import scipy.io\n",
        "import biosppy\n",
        "import mne\n",
        "import neurokit2 as nk\n",
        "import cvxopt as cv\n",
        "import cvxopt.solvers\n",
        "import ast\n",
        "import os\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import glob\n",
        "from scipy.stats import zscore, norm\n",
        "from neurokit2 import eda_phasic\n",
        "from scipy.stats import linregress\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QExRhAIs_3hj"
      },
      "source": [
        "# this function is to convert the TimeStamp column (first column) from Unix Epoch time to standard datetime format\n",
        "def TimeStamp_Conversion(ts):\n",
        "  \"\"\"\n",
        "  we have a unix epoch time in milliseconds i.e, a string with a length of 13 charcters example:1.5789360034388428E12\n",
        "  \n",
        "  parameters:\n",
        "  -----\n",
        "  ts = Epoch timesatmp in milliseconds.\n",
        "\n",
        "  Returns:\n",
        "  -----\n",
        "  Std_Unix = standard epoch timestamp in seconds.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  float_Input = float(ts) # converting the string-type(1.5789360034388428E12) Unix Epoch to float-type(1578936003.4388428).\n",
        "\n",
        "  # float input is divided by 1000 to convert the Unix epoch in milliseconds to seconds \n",
        "  Std_Unix = float_Input/1000\n",
        "\n",
        "  datetime_Input = datetime.fromtimestamp(Std_Unix) \n",
        "  # datetime.fromtimestamp converts the unix epoch in seconds to datetime returns example:datetime.datetime(2020, 1, 13, 17, 20, 3, 438843)\n",
        "\n",
        "  return Std_Unix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXnhE_ePAarf"
      },
      "source": [
        "def column_formatting(Timestamp_DF):\n",
        "  \"\"\"\n",
        "  Column names of Timestamp annotation excel have column index attached to column name as we only need column name we are parsing column names.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  Timestamp_DF = Input the DF after reading the timestamp annotationexcel file to  get list of column names['A1- ECG baseline start','B1- ECG baseline end',.....].\n",
        "\n",
        "  Returns:\n",
        "  -----\n",
        "  Parsed_ColumnNames = list of parsed column names. ['ECG baseline start','ECG baseline end',....]\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Parsed_colnames = ['Subject_ID'] ## Column with Participant ID is not named, so declaring first column as Subject_ID to an empty list\n",
        "\n",
        "  for index in range(1,len(Timestamp_DF.columns)): ## Looping through the list of timestamp annotation columns list\n",
        "    column = Timestamp_DF.columns[index][4:].lstrip() ## Drop first 3 indices of each column and strip space(\" \") if present as left most\n",
        "    Parsed_colnames.append(column) ## appending each column name after parsing\n",
        "\n",
        "  return Parsed_colnames ## returns list fo parsed col names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrGsYy1cFLYc"
      },
      "source": [
        "def Annotation_timestamp(timestamp_path, sheet_name):\n",
        "  \"\"\"\n",
        "  This function is to change the column names of timestamp annotations table and convert timestamps from milliseconds to standart epoch format of seconds.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  timestamp_path = path to the directory of file location\n",
        "  sheet_name =  there are two sheets present in the file, we work on file named D.\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  VR_Timestamps_D = Clean dataframe of timestamp annotations table.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  VR_TimeStamps_D = pd.read_excel(Timestamp_path, sheet_name) ## read timestamp annotation file\n",
        "  Parsed_colnames = column_formatting(VR_TimeStamps_D) ## using the column_formatting function defined earlier parse columns\n",
        "  VR_TimeStamps_D.columns = Parsed_colnames ## Change colnames of Dataframe using the parsed list of col names\n",
        "  \n",
        "  ## As timestamp is in string format and in milli seconds iterating through each column to change the timestamp to standard epoch format.\n",
        "  for col in VR_TimeStamps_D.columns: \n",
        "    ## Using Timestamp_Conversion function and lambda fucntion to map the function to each row of the column.\n",
        "    if col == 'Subject_ID':\n",
        "      pass\n",
        "    else:\n",
        "      VR_TimeStamps_D[col] = VR_TimeStamps_D[col].map(lambda instance: TimeStamp_Conversion(instance)) \n",
        "\n",
        "  return VR_TimeStamps_D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xihq4RSTAas4"
      },
      "source": [
        "def Shimmers_csv2DF(path,filename):\n",
        "  \"\"\"\n",
        "  This function is to read Shimmer data files and create a dataframe from tidy shimmers csv tables.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  path = path to directory of shimmers file folder.\n",
        "\n",
        "  filename = name of the file to be loaded.\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  Dataframe = organized and structured Shimmers Data.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  with open(path + '/' + filename, 'r',) as file: # read the file\n",
        "    reader = csv.reader(file)\n",
        "\n",
        "    lists_eachrow = []\n",
        "    for row in reader:\n",
        "      lists_eachrow.append(row) # append each row in reader to a list\n",
        "\n",
        "  del lists_eachrow[0] # del first row of list as it is only about \\t delimiter used\n",
        "\n",
        "  newlists = [] \n",
        "  # loop through the list of lists and split columnar values using the delimiter \n",
        "  for list_row in lists_eachrow:\n",
        "    for row in list_row:\n",
        "      newlists.append(list(row.split('\\t')))\n",
        "  # Extract subjectID from the file name for future use\n",
        "  filename_parse = filename.replace(\"_\", \" \")\n",
        "  Participant_ID = ast.literal_eval(re.findall(r'\\b\\d+\\b', filename_parse)[0])\n",
        "  \n",
        "  # create dataframe from the list of columnar values \n",
        "  Dataframe = pd.DataFrame(newlists, columns = newlists[0])\n",
        "  Dataframe = Dataframe.drop([0,1]) # drop columns 1 and 2 as are column names and units which we already have for new dataframe.\n",
        "  Dataframe.reset_index(drop=True, inplace=True) # reset index\n",
        "\n",
        "  return Dataframe, Participant_ID "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXR9Vr1dAawB"
      },
      "source": [
        "def Sliding_Window_GSRFeatureExt(GSR_16Hz,Participant_ID, start_window, overlap, window_length):\n",
        "  \"\"\"\n",
        "  This function is to create a dataframe consisting of statistical features extracted using sliding window technique on Phasic and Tonic components of GSR signal.\n",
        "\n",
        "  Parameters:\n",
        "  -----\n",
        "  GSR_16Hz = This the Dataframe consisting of Phasic and Tonic components of GSR signal along with epoch timestamp of each instance.\n",
        "\n",
        "  Participant_ID = Subject ID extracted in Shimmers_csv2DF function from the file name.\n",
        "  \n",
        "  start_window = 0, starting index of the sliding window\n",
        "  \n",
        "  overlap = 50, sliding window tech. with 50 overlap\n",
        "  \n",
        "  window_length = 100, length of each window (100 rows at once)\n",
        "\n",
        "  Results:\n",
        "  -----\n",
        "  Dataframe consisting of statistical features like Mean Phasic&Tonic components, std Phasic&Tonic components, Count and Mean of Phasic&Tonic Peaks, \n",
        "  Min and Max, Slope, AUC of Phasic&Tonic components for each window of length 100 and overlap of 50.\n",
        "\n",
        " \"\"\"\n",
        "  # creating an array of phasic, tonic and timestamps from the GSR_16Hz dataframe, will be easy to calculate statistical features of numpy array.\n",
        "  phasic  = np.array(GSR_16Hz['phasic']) \n",
        "  tonic  = np.array(GSR_16Hz['tonic'])\n",
        "  timestamp = np.array(GSR_16Hz['TimeStamp'])\n",
        "  dummy = list(range(0,len(GSR_16Hz))) # this is to use as secondary axis to calculate slope.\n",
        "\n",
        "  end_window = window_length\n",
        "  Start_timestamp = []\n",
        "  Mean_timestamp = []\n",
        "  end_timestamp = []\n",
        "  Mean_phasic = []\n",
        "  Mean_tonic = []\n",
        "  Std_phasic = []\n",
        "  Std_tonic = []\n",
        "  CountPeak_phasic = []\n",
        "  CountPeak_tonic = []\n",
        "  MeanPeak_phasic = []\n",
        "  MeanPeak_tonic = []\n",
        "  Min_phasic = []\n",
        "  Max_phasic = []\n",
        "  Min_tonic = []\n",
        "  Max_tonic = []\n",
        "  Slope_tonic = []\n",
        "  Slope_phasic = []\n",
        "  AUC_tonic = []\n",
        "  AUC_phasic = []\n",
        "  while abs(end_window-window_length) <= len(GSR_16Hz): # loop conditon to carry out windowing and calculate features till the end \n",
        "\n",
        "    # creating start, mean and end timestamps helps to  make sure that each window completely falls in annotation period\n",
        "    Start_timestamp.append(timestamp[start_window]) \n",
        "    Mean_timestamp.append(np.mean(timestamp[start_window:end_window]))\n",
        "    try:\n",
        "      end_timestamp.append(timestamp[end_window])\n",
        "    except IndexError: \n",
        "      # index of last window might exceed the last timestamp of our data, if this happens it will use last timestamp of data as end timsatmp of last window\n",
        "      end_timestamp.append(timestamp[-1])\n",
        "\n",
        "    # using numpy to compute statistucal features like mean, std, count\n",
        "    Mean_phasic.append(np.mean(phasic[start_window:end_window]))\n",
        "    Mean_tonic.append(np.mean(tonic[start_window:end_window]))\n",
        "    Std_phasic.append(np.std(phasic[start_window:end_window]))\n",
        "    Std_tonic.append(np.std(tonic[start_window:end_window]))\n",
        "    # scipy.signal.find_peaks exctracts the peaks present in teh signal\n",
        "    CountPeak_phasic.append(scipy.signal.find_peaks(phasic[start_window:end_window])[0].size)\n",
        "    CountPeak_tonic.append(scipy.signal.find_peaks(tonic[start_window:end_window])[0].size)\n",
        "    MeanPeak_phasic.append(np.mean(scipy.signal.find_peaks(phasic[start_window:end_window])[0]))\n",
        "    MeanPeak_tonic.append(np.mean(scipy.signal.find_peaks(tonic[start_window:end_window])[0]))\n",
        "    Min_phasic.append(min(phasic[start_window:end_window]))\n",
        "    Max_phasic.append(max(phasic[start_window:end_window]))\n",
        "    Min_tonic.append(min(tonic[start_window:end_window]))\n",
        "    Max_tonic.append(max(tonic[start_window:end_window]))\n",
        "    # slope is computed using linregress module\n",
        "    Slope_tonic.append(linregress(dummy[start_window:end_window],tonic[start_window:end_window])[0])\n",
        "    Slope_phasic.append(linregress(dummy[start_window:end_window],phasic[start_window:end_window])[0])\n",
        "    # Area under curve is calculated using sk learn metrics\n",
        "    AUC_tonic.append(metrics.auc(dummy[start_window:end_window],tonic[start_window:end_window]))\n",
        "    AUC_phasic.append(metrics.auc(dummy[start_window:end_window],phasic[start_window:end_window]))\n",
        "      \n",
        "    # to increment start and end for next window\n",
        "    start_window += overlap\n",
        "    end_window += overlap\n",
        "  \n",
        "  # create Data frame from the lists of values appended to each feature.\n",
        "  Feature_DF = pd.DataFrame()\n",
        "  Feature_DF['Start_timestamp'] = Start_timestamp\n",
        "  Feature_DF['Mean_timestamp'] = Mean_timestamp\n",
        "  Feature_DF['End_timestamp'] = end_timestamp\n",
        "  Feature_DF['Mean_phasic'] = Mean_phasic\n",
        "  Feature_DF['Mean_tonic'] = Mean_tonic\n",
        "  Feature_DF['Std_phasic'] = Std_phasic\n",
        "  Feature_DF['Std_tonic'] = Std_tonic\n",
        "  Feature_DF['CountPeak_phasic'] = CountPeak_phasic\n",
        "  Feature_DF['CountPeak_tonic'] = CountPeak_tonic\n",
        "  Feature_DF['MeanPeak_phasic'] = MeanPeak_phasic\n",
        "  Feature_DF['MeanPeak_tonic'] = MeanPeak_tonic\n",
        "  Feature_DF['Min_phasic'] = Min_phasic\n",
        "  Feature_DF['Max_phasic'] = Max_phasic\n",
        "  Feature_DF['Min_tonic'] = Min_tonic\n",
        "  Feature_DF['Max_tonic'] = Max_tonic\n",
        "  Feature_DF['Slope_tonic'] = Slope_tonic\n",
        "  Feature_DF['Slope_phasic'] = Slope_phasic\n",
        "  Feature_DF['AUC_tonic'] = AUC_tonic\n",
        "  Feature_DF['AUC_phasic'] = AUC_phasic\n",
        "  Feature_DF['Subject_ID'] = Participant_ID\n",
        "\n",
        "  return Feature_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1J-elgp-SL5"
      },
      "source": [
        "def Tonic_col(columns):\n",
        "  tonic_col = []\n",
        "  for x in columns:\n",
        "    if 'tonic' in x:\n",
        "      tonic_col.append(x)\n",
        "  return tonic_col"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}